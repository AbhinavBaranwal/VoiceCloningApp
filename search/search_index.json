{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Voice Cloning App A Python/Pytorch app for easily synthesising human voices. Getting started After installing & running the app it should open in your browser at localhost:5000 Windows Download the latest release from releases If you have an NVIDIA GPU make sure to download the executable with GPU support Otherwise, download the 'cpuonly' version Linux Clone this repository Run ./install.sh from the root of the repository Run python3.6 main.py Manual Install (Linux/ Windows) Users familiar with Python may prefer building the app themselves: Clone this repository Install Python (version 3.6) Run pip install -r requirements.txt (or pip install -r requirements-cpu.txt for the 'cpuonly' build) Run python main.py Adding languages If you are using a language other than English you can add it to the app. Firstly, you'll need to find a deepspeech model for your language by going to coqui . You'll then need to download the model.pbmm and alphabet.txt files for your language. To add this to the app go to \"Settings\" -> \"Add a language\", enter your language name and upload your language files Please note that language model quality may vary and so successful generation of voices in other languages is not guarenteed.","title":"Setup"},{"location":"#welcome-to-the-voice-cloning-app","text":"A Python/Pytorch app for easily synthesising human voices.","title":"Welcome to the Voice Cloning App"},{"location":"#getting-started","text":"After installing & running the app it should open in your browser at localhost:5000","title":"Getting started"},{"location":"#windows","text":"Download the latest release from releases If you have an NVIDIA GPU make sure to download the executable with GPU support Otherwise, download the 'cpuonly' version","title":"Windows"},{"location":"#linux","text":"Clone this repository Run ./install.sh from the root of the repository Run python3.6 main.py","title":"Linux"},{"location":"#manual-install-linux-windows","text":"Users familiar with Python may prefer building the app themselves: Clone this repository Install Python (version 3.6) Run pip install -r requirements.txt (or pip install -r requirements-cpu.txt for the 'cpuonly' build) Run python main.py","title":"Manual Install (Linux/ Windows)"},{"location":"#adding-languages","text":"If you are using a language other than English you can add it to the app. Firstly, you'll need to find a deepspeech model for your language by going to coqui . You'll then need to download the model.pbmm and alphabet.txt files for your language. To add this to the app go to \"Settings\" -> \"Add a language\", enter your language name and upload your language files Please note that language model quality may vary and so successful generation of voices in other languages is not guarenteed.","title":"Adding languages"},{"location":"dataset/","text":"Dataset A voice dataset consists of audio (of the target voice) and text (labelling what is in said in the audio). Selecting a source Before creating a voice dataset it is important to ensure you've selected a suitable candidate for which you can obtain audio file(s) and text transcriptions (either plaintext or subtitles). The most common source is audiobooks because the audio is clear and labelled, but you may be able to extract from other sources such as podcasts. Check the criteria in Verify your source to determine whether your given source will be suitable. Extracting from audible/kindle Audible & Kindle are a great source for gathering the audio & text data needed for the voice dataset. To build a data source, firstly purchase a matching audible & kindle book (typically referred to whispersync on amazon). Then do the following to extract the audio & text: Audible to audio To convert an audible audio book to an audio file (.mp3) I use AaxAudioConverter . Here are the steps to install and use: Download and install the Windows Audible app Open the audible app and download the audiobook you wish to convert Download AaxAudioConverter Run the application and click \"Add\" in the top left Select the audiobook you downloaded in the audible app Select \"MP3\" as the export format and \"Single file per AAX file\". Then click convert Kindle to text To convert a kindle book to text file (.txt): Download and install Kindle App version 1.17 (NOTE: It must be this version or earlier to work) Open the kindle app and download the book you wish to convert Go to \"Documents\\My Kindle Content\" and find your kindle book (.azw file) Upload this file to Convert to a txt If this doesn't work you can also follow the DeDRM guide Verify your source Audio Check the following about your source audio before starting: The audio is clear (without background noise) The audio only includes the target speaker (the voice of the person you want to clone) The total duration of the audio file(s) is several hours long In some cases less audio may work, but this is not advised If your audio does not meet any of these requirements, it is unlikely to be successful. Text/Subtitles Check the following about your source text before starting: The text accurately labels what is said in the audio The text includes punctuation If your text does not accurately match your audio, the dataset cannot be built. Building a dataset Using the app To build a dataset in the app you can simply upload your source audio/text and enter a name for the dataset. If using another language you can select this here (to add a language see Adding languages ) Once you do this a progress bar will appear, meaning that the dataset building process has started. You must wait for this progress bar to complete and the 'Next' button to appear before continuing. Your dataset has now been generated. Manually creating a dataset If for whatever reason you cannot use the dataset generator or want to create your dataset manually, this is possible. To do so you'll need to extract 1-10 second clips from your audio (I would suggest using an app such as Audacity to do this). Save these clips to a folder called wavs inside your dataset folder. Next, you'll need to create the labels file. To do this create a file called metadata.csv and add what is said in each clip in the format filename|transcription . For example: Clip1.wav|Hi, my name is Dave? Clip2.wav|Great weather we're having. ... This leaves the final format of the dataset folder to be something like this: metadata.csv # The labels file wavs/ # The audio clips folder Clip1.wav Clip2.wav ... You can then import the dataset into the app by zipping it and adding under the \"Import/Export\" menu -> \"Upload dataset\".","title":"Dataset"},{"location":"dataset/#dataset","text":"A voice dataset consists of audio (of the target voice) and text (labelling what is in said in the audio).","title":"Dataset"},{"location":"dataset/#selecting-a-source","text":"Before creating a voice dataset it is important to ensure you've selected a suitable candidate for which you can obtain audio file(s) and text transcriptions (either plaintext or subtitles). The most common source is audiobooks because the audio is clear and labelled, but you may be able to extract from other sources such as podcasts. Check the criteria in Verify your source to determine whether your given source will be suitable.","title":"Selecting a source"},{"location":"dataset/#extracting-from-audiblekindle","text":"Audible & Kindle are a great source for gathering the audio & text data needed for the voice dataset. To build a data source, firstly purchase a matching audible & kindle book (typically referred to whispersync on amazon). Then do the following to extract the audio & text: Audible to audio To convert an audible audio book to an audio file (.mp3) I use AaxAudioConverter . Here are the steps to install and use: Download and install the Windows Audible app Open the audible app and download the audiobook you wish to convert Download AaxAudioConverter Run the application and click \"Add\" in the top left Select the audiobook you downloaded in the audible app Select \"MP3\" as the export format and \"Single file per AAX file\". Then click convert Kindle to text To convert a kindle book to text file (.txt): Download and install Kindle App version 1.17 (NOTE: It must be this version or earlier to work) Open the kindle app and download the book you wish to convert Go to \"Documents\\My Kindle Content\" and find your kindle book (.azw file) Upload this file to Convert to a txt If this doesn't work you can also follow the DeDRM guide","title":"Extracting from audible/kindle"},{"location":"dataset/#verify-your-source","text":"","title":"Verify your source"},{"location":"dataset/#audio","text":"Check the following about your source audio before starting: The audio is clear (without background noise) The audio only includes the target speaker (the voice of the person you want to clone) The total duration of the audio file(s) is several hours long In some cases less audio may work, but this is not advised If your audio does not meet any of these requirements, it is unlikely to be successful.","title":"Audio"},{"location":"dataset/#textsubtitles","text":"Check the following about your source text before starting: The text accurately labels what is said in the audio The text includes punctuation If your text does not accurately match your audio, the dataset cannot be built.","title":"Text/Subtitles"},{"location":"dataset/#building-a-dataset","text":"","title":"Building a dataset"},{"location":"dataset/#using-the-app","text":"To build a dataset in the app you can simply upload your source audio/text and enter a name for the dataset. If using another language you can select this here (to add a language see Adding languages ) Once you do this a progress bar will appear, meaning that the dataset building process has started. You must wait for this progress bar to complete and the 'Next' button to appear before continuing. Your dataset has now been generated.","title":"Using the app"},{"location":"dataset/#manually-creating-a-dataset","text":"If for whatever reason you cannot use the dataset generator or want to create your dataset manually, this is possible. To do so you'll need to extract 1-10 second clips from your audio (I would suggest using an app such as Audacity to do this). Save these clips to a folder called wavs inside your dataset folder. Next, you'll need to create the labels file. To do this create a file called metadata.csv and add what is said in each clip in the format filename|transcription . For example: Clip1.wav|Hi, my name is Dave? Clip2.wav|Great weather we're having. ... This leaves the final format of the dataset folder to be something like this: metadata.csv # The labels file wavs/ # The audio clips folder Clip1.wav Clip2.wav ... You can then import the dataset into the app by zipping it and adding under the \"Import/Export\" menu -> \"Upload dataset\".","title":"Manually creating a dataset"},{"location":"synthesis/","text":"Synthesis Once you've trained a voice model, you can produced synthesised audio. Add a vocoder Vocoder's are the models that convert the results generated by your voice model into audio. The app currently supports Hifigan models which can be imported into the app in \"Settings\" -> \"Add a vocoder\". You will need to enter a name for the vocoder and upload the g_02500000 model and config.json which can be downloaded from the sample link provided Synthesize After you have a voice model and a vocoder you can synthesize results. The first step in this process is to select your voice name, checkpoint (this is the voice model, choose the most recent version), language and vocoder. Once you've done this you can now enter the text you want to produce results for. To do this there are 3 options: Single line: A single sentence you want to produce reults for Multi line: Multiple sentences produced together and then joined Paragraph: A paragraph of text split automatically into multiple sentences which are then synthesized and joined If you don't know where to start, just choose \"Single line\" and enter a simple sentence like \"This is a sample sentence.\". Please note: If your sentence is too short (i.e. < 5 words) or too long (i.e. > 20 words) then synthesis may not work Once you press submit, results should be generated. From here you can view the alignment graph which shows how well the text & audio have aligned (this should form a relatively clear diagonal line) and listen to the audio. You can also modify the text at the bottom and press submit to generate more synthesised results. If audio could not be produced or was poor this may be due to the given text (either being too short/long or containing difficult words for your voice) or poor voice quality","title":"Synthesis"},{"location":"synthesis/#synthesis","text":"Once you've trained a voice model, you can produced synthesised audio.","title":"Synthesis"},{"location":"synthesis/#add-a-vocoder","text":"Vocoder's are the models that convert the results generated by your voice model into audio. The app currently supports Hifigan models which can be imported into the app in \"Settings\" -> \"Add a vocoder\". You will need to enter a name for the vocoder and upload the g_02500000 model and config.json which can be downloaded from the sample link provided","title":"Add a vocoder"},{"location":"synthesis/#synthesize","text":"After you have a voice model and a vocoder you can synthesize results. The first step in this process is to select your voice name, checkpoint (this is the voice model, choose the most recent version), language and vocoder. Once you've done this you can now enter the text you want to produce results for. To do this there are 3 options: Single line: A single sentence you want to produce reults for Multi line: Multiple sentences produced together and then joined Paragraph: A paragraph of text split automatically into multiple sentences which are then synthesized and joined If you don't know where to start, just choose \"Single line\" and enter a simple sentence like \"This is a sample sentence.\". Please note: If your sentence is too short (i.e. < 5 words) or too long (i.e. > 20 words) then synthesis may not work Once you press submit, results should be generated. From here you can view the alignment graph which shows how well the text & audio have aligned (this should form a relatively clear diagonal line) and listen to the audio. You can also modify the text at the bottom and press submit to generate more synthesised results. If audio could not be produced or was poor this may be due to the given text (either being too short/long or containing difficult words for your voice) or poor voice quality","title":"Synthesize"},{"location":"training/","text":"Training Training can be done locally (if you have an NVIDIA GPU with more than 4GB of memory) or remotely (on Google colab ) Local training Before starting training you'll need to set various parameters Select your dataset You can select your dataset from the dropdown on the training form. When you select a dataset, a message will appear underneath the dropdown which tells you it's total size. This is important, because a dataset that is too small is unlikely to work. If your dataset is invalid or has been corrupted an error will show instead. In this case you will need to recreate or fix the dataset. Parameters In addition to selecting the dataset you'll need to set 3 required parameters: Pretrained model: This is a file that helps the model to generate high quality results. The recommended option is the use the default provded by NVIDIA. This can be downloaded from the \"default\" link Language: The language used in the dataset Epochs: This is the number of cycles over the dataset. Increasing this will increase quality (to a point) but also increases training time Smaller datasets need less epochs to reach max quality than bigger ones The reccommended approach is to start with a small amount (say 500) and then revisit training to continue until you are happy with the results Once you reach the maximum quality of your voice, continuing to train will not improve results and may make this worse \"CUDA Out of Memory error\" A common error during training is \"CUDA of of memory\". What this means is that your GPU ran out of memory whilst training. To resolve this you need to decrease the batch size which can be found under Advanced Options. Do this a little at a time and retry until training works successfully. During training During training you'll see the standard progress bar which indicates the training progress. Under the progress bar you'll also see a message which includes some important stats: Train loss: The loss score indicates how well the model is learning the training data. A low score is best but remember this can be misleading as a low score is also possible for overfitting Attention score: A score which roughly represents how well text is linking to audio. Higher the better Remember: If you want to stop and continue later this is not a problem as checkpoints are saved frequently. Additionally, if you set the \"Alignment text\" in Advanced Options to a test sentence you'll see an alignment graph each time a checkpoint is saved. This shows how well the text/aduio is aligning, with close to diagonal line being ideal Continuing training Training can be stopped at any time and resumed later. When you revisit training you'll see a dropdown to select the checkpoint to resume training from. You should select the latest checkpoint when doing so. This will restore your training to the point it was at when the checkpoint was created. The frequency that checkpoints are saved can be set in the \"advanced options\" of training. Remote training Remote training works the same as local training but executes in Google colab to use a remote GPU. Exporting the dataset You'll firstly need to export your dataset from the app to upload to the notebook. You can do so by selecting your dataset from the dropdown and clicking \"Export\". You can then click \"Open notebook\" to launch the notebook. To add your dataset, you'll need to unzip and upload the folder to your google drive under the path Voice-Cloning/datasets . If you have already run the notebook as far as the step where you conenct to the google drive, you'll notice this folder already exists. You can then select the dataset by entering its name into the 3rd cell. Parameters Parameters can be set by changing the values in the 4th cell. Note that you'll need to set these before you run the cell or they won't work. Unlike local training, you will not need to upload a pretrained model as this is done for you automatically. Checkpoints Checkpoints are created just like in local training and are saved to Voice-Cloning/checkpoints in your google drive. When you want to try the checkpoint out, simply download it and import it into the app under \"Import/Export\" -> \"Upload model\" Verifying quality To identify whether you have produced a good voice model you can stop training at any time and test your model in synthesis However, you can also tell whether your model quality will be good by looking at a variety of other factors: Train loss: Did your train loss decrease to a consistent value (typically < 0.5)? Attention score: Did your attention score increase to a consistent value (typically > 0.3)? Alignment graph: If you enabled the alignment graph (by using an \"Alignment sentence\" in advanced settings) did this form a relatively clear line? If any of these things are not true, then your model may not be good quality. There are a number of possible reasons for this: Your dataset was not good enough quality (see Verify your source for possible reasons why) You did not train for long enough Did you upload a transfer learning model? Did you train for 500+ epochs?","title":"Training"},{"location":"training/#training","text":"Training can be done locally (if you have an NVIDIA GPU with more than 4GB of memory) or remotely (on Google colab )","title":"Training"},{"location":"training/#local-training","text":"Before starting training you'll need to set various parameters","title":"Local training"},{"location":"training/#select-your-dataset","text":"You can select your dataset from the dropdown on the training form. When you select a dataset, a message will appear underneath the dropdown which tells you it's total size. This is important, because a dataset that is too small is unlikely to work. If your dataset is invalid or has been corrupted an error will show instead. In this case you will need to recreate or fix the dataset.","title":"Select your dataset"},{"location":"training/#parameters","text":"In addition to selecting the dataset you'll need to set 3 required parameters: Pretrained model: This is a file that helps the model to generate high quality results. The recommended option is the use the default provded by NVIDIA. This can be downloaded from the \"default\" link Language: The language used in the dataset Epochs: This is the number of cycles over the dataset. Increasing this will increase quality (to a point) but also increases training time Smaller datasets need less epochs to reach max quality than bigger ones The reccommended approach is to start with a small amount (say 500) and then revisit training to continue until you are happy with the results Once you reach the maximum quality of your voice, continuing to train will not improve results and may make this worse","title":"Parameters"},{"location":"training/#cuda-out-of-memory-error","text":"A common error during training is \"CUDA of of memory\". What this means is that your GPU ran out of memory whilst training. To resolve this you need to decrease the batch size which can be found under Advanced Options. Do this a little at a time and retry until training works successfully.","title":"\"CUDA Out of Memory error\""},{"location":"training/#during-training","text":"During training you'll see the standard progress bar which indicates the training progress. Under the progress bar you'll also see a message which includes some important stats: Train loss: The loss score indicates how well the model is learning the training data. A low score is best but remember this can be misleading as a low score is also possible for overfitting Attention score: A score which roughly represents how well text is linking to audio. Higher the better Remember: If you want to stop and continue later this is not a problem as checkpoints are saved frequently. Additionally, if you set the \"Alignment text\" in Advanced Options to a test sentence you'll see an alignment graph each time a checkpoint is saved. This shows how well the text/aduio is aligning, with close to diagonal line being ideal","title":"During training"},{"location":"training/#continuing-training","text":"Training can be stopped at any time and resumed later. When you revisit training you'll see a dropdown to select the checkpoint to resume training from. You should select the latest checkpoint when doing so. This will restore your training to the point it was at when the checkpoint was created. The frequency that checkpoints are saved can be set in the \"advanced options\" of training.","title":"Continuing training"},{"location":"training/#remote-training","text":"Remote training works the same as local training but executes in Google colab to use a remote GPU.","title":"Remote training"},{"location":"training/#exporting-the-dataset","text":"You'll firstly need to export your dataset from the app to upload to the notebook. You can do so by selecting your dataset from the dropdown and clicking \"Export\". You can then click \"Open notebook\" to launch the notebook. To add your dataset, you'll need to unzip and upload the folder to your google drive under the path Voice-Cloning/datasets . If you have already run the notebook as far as the step where you conenct to the google drive, you'll notice this folder already exists. You can then select the dataset by entering its name into the 3rd cell.","title":"Exporting the dataset"},{"location":"training/#parameters_1","text":"Parameters can be set by changing the values in the 4th cell. Note that you'll need to set these before you run the cell or they won't work. Unlike local training, you will not need to upload a pretrained model as this is done for you automatically.","title":"Parameters"},{"location":"training/#checkpoints","text":"Checkpoints are created just like in local training and are saved to Voice-Cloning/checkpoints in your google drive. When you want to try the checkpoint out, simply download it and import it into the app under \"Import/Export\" -> \"Upload model\"","title":"Checkpoints"},{"location":"training/#verifying-quality","text":"To identify whether you have produced a good voice model you can stop training at any time and test your model in synthesis However, you can also tell whether your model quality will be good by looking at a variety of other factors: Train loss: Did your train loss decrease to a consistent value (typically < 0.5)? Attention score: Did your attention score increase to a consistent value (typically > 0.3)? Alignment graph: If you enabled the alignment graph (by using an \"Alignment sentence\" in advanced settings) did this form a relatively clear line? If any of these things are not true, then your model may not be good quality. There are a number of possible reasons for this: Your dataset was not good enough quality (see Verify your source for possible reasons why) You did not train for long enough Did you upload a transfer learning model? Did you train for 500+ epochs?","title":"Verifying quality"}]}